{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Student Admissions with Neural Networks\n",
    "In this notebook, we predict student admissions to graduate school at UCLA based on three pieces of data:\n",
    "\n",
    "    -GRE Scores (Test)\n",
    "    -GPA Scores (Grades)\n",
    "    -Class rank (1-4)\n",
    "\n",
    "The dataset originally came from here: http://www.ats.ucla.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calss handles all the procedure from loading data to testing accuracy\n",
    "class NN:\n",
    "    # constructor to load data from csv file\n",
    "    def __init__(self, file):\n",
    "        np.random.seed(20)  # to make sure, random is most random all the time\n",
    "        self.__data = pd.read_csv(file)\n",
    "        print('Data Loaded')\n",
    "    \n",
    "    \n",
    "    # show data\n",
    "    def getData(self):\n",
    "        return self.__data\n",
    "    \n",
    "    \n",
    "    # One-hot encode\n",
    "    # General function to apply one-hot encode on any data, just pass the dataset and col to convert to one-hot encoded vector\n",
    "    def one_hot_encoder(self, col):\n",
    "        classes = np.sort(self.__data[col].unique())\n",
    "        one_hot_vector = np.array([[0  if val != c else 1 for c in classes] for val in self.__data[col]])\n",
    "        self.__data = pd.merge(\n",
    "            self.__data,\n",
    "            pd.DataFrame(data=one_hot_vector, columns=classes),\n",
    "            left_index=True, \n",
    "            right_index=True\n",
    "        )\n",
    "        self.__data.drop([col], axis=1, inplace=True)\n",
    "        print('Done! One Hot Encoding on {} column'.format(col))\n",
    "    \n",
    "    \n",
    "    # Scaling the data\n",
    "    # We notice that the range for grades is 1.0-4.0, \n",
    "    # whereas the range for test scores is roughly 200-800, which is much larger. \n",
    "    # This means our data is skewed, and that makes it hard for a neural network to handle. \n",
    "    # Let's fit our two features into a range of 0-1, by dividing the grades by 4.0, and the test score by 800.\n",
    "    # But below will follow for any kind of data\n",
    "    def scale_data(self, *cols):\n",
    "        for col in cols:\n",
    "            self.__data[col] = self.__data[col]/np.max(self.__data[col])\n",
    "        print('Scaling Done')\n",
    "    \n",
    "    \n",
    "    # Splitting the data into training and testing \n",
    "    # further splitting the data into features(X) and lables(y)\n",
    "    # As we want random values from dataset to be selected as sample, \n",
    "    # we will use np.random.choice() to select random indices for sample\n",
    "    def train_test_split(self, label_col):\n",
    "        sample = np.random.choice(self.__data.index, size=int(len(self.__data)*0.8), replace=False)\n",
    "        train_data, test_data = self.__data.iloc[sample], self.__data.drop(sample)\n",
    "        print('No. of rows in training data: ', len(train_data))\n",
    "        print('No. of rows in testing data: ', len(test_data))\n",
    "        self.__features = np.array(train_data.drop([label_col], axis=1))\n",
    "        self.__labels = np.array(train_data[label_col])\n",
    "        self.__features_test = np.array(test_data.drop([label_col], axis=1))\n",
    "        self.__labels_test = np.array(test_data[label_col])\n",
    "    \n",
    "    \n",
    "    # activation function\n",
    "    # previous activation is inputs and weights is current thetas\n",
    "    # x = np.dot(inputs, weight)\n",
    "    # or x = np.dot(previous_activation, current_thetas)\n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    \n",
    "    # derivative of loss function in forward pass\n",
    "    def delForward(self, x, y):\n",
    "        return np.matmul(x.T, y*(1-y))\n",
    "    \n",
    "    \n",
    "    # Define layers of neural network\n",
    "    # in neurons enter a list \n",
    "    #?--> # e.g. [input_features/neurons, hidden_layer_1_neurons, hidden_layer_2_neurons, output_layer_neurons]\n",
    "    def layers(self, neurons):\n",
    "        self.__neurons = neurons\n",
    "        print('Layers of Neural Network:')\n",
    "        print('Layer 1, Input Layer Neurons: {}'.format(self.__neurons[0]))\n",
    "        for i in range(1, len(self.__neurons)-1):\n",
    "            print('Layer {}, Hidden Layer {} Neurons: {}'.format(i+1, i, self.__neurons[i]))\n",
    "        print('Layer {}, Output Layer Neurons: {}'.format(len(self.__neurons), self.__neurons[-1]))\n",
    "        \n",
    "    \n",
    "    # forward pass\n",
    "    def __forward(self):\n",
    "        for i in range(len(self.__neurons)-1):\n",
    "            self.__activations.append(sigmoid(np.dot(self.__activations[i], weights[i+1])))\n",
    "            self.__del_forward.append(delForward(self.__activations[i], self.__activations[i+1]))\n",
    "    \n",
    "    \n",
    "    # reverse/backward pass\n",
    "    def __backward(self):\n",
    "        for i in range(len(self.__neurons)-2, 0, -1):\n",
    "            self.__del_backward.append(self.__weights[i+1], self.__del_backward[i+1])\n",
    "        \n",
    "    \n",
    "    # update weights\n",
    "    def __updateWeights(self):\n",
    "        for i in range(len(self.__neurons)):\n",
    "            weights[i] -= aplha*(self.__del_forward*sel.__del_backward.T) \n",
    "        \n",
    "    \n",
    "    # return weights\n",
    "    def getWeights(s)\n",
    "    # Training Neural Network\n",
    "    def train_nn(self, epochs=1000, alpha=0.1, batch_size=64):\n",
    "        # n_records, n_features = features.shape\n",
    "        last_loss = None\n",
    "        \n",
    "        # initialze weights/thetas for all the layers\n",
    "        self.__weights = [] \n",
    "        for i in range(len(self.__neurons)-1):\n",
    "            self.__weights.append(np.random.normal(scale=self.__neurons[i]**-.5, size=(self.__neurons[i], self.__neurons[i+1])))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            indices = np.random.choice(self.__features, size=(batch_size, features[0]), replace=False)\n",
    "            \n",
    "            # forward\n",
    "            self.__activations = [self.__features[indices]]\n",
    "            __forward()\n",
    "            \n",
    "            # backward\n",
    "            self.__E = np.mean(self.__activations[-1] - labels[indices])\n",
    "            self.__del_backward = [self.__E, self.__E*self.__weights[-1]]\n",
    "            __backward()\n",
    "            __updateWeights()\n",
    "            \n",
    "            if e % (epochs/10) == 0:\n",
    "                out = sigmoid(np.dot(features, weights))\n",
    "                loss = np.mean((out - labels)**2)\n",
    "                print('Epoch:', e)\n",
    "                if e == 0:\n",
    "                    last_loss = loss\n",
    "                if last_loss < loss:\n",
    "                    print('Train loss:', loss, \"WARNING - Loss Increasing\")\n",
    "                else:\n",
    "                    print('Train loss:', loss)\n",
    "                last_loss = loss\n",
    "                print(\"============\")\n",
    "        print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "obj = NN('student_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11693129, -0.35895849,  0.29031635, -0.4534367 , -0.88931928,\n",
       "       -0.66072138])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "test_output = sigmoid(np.dot(features_test, weights))\n",
    "predictions = test_output > 0.5\n",
    "accuracy = np.mean(predictions == labels_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
