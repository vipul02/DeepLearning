{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Student Admissions with Neural Networks\n",
    "In this notebook, we predict student admissions to graduate school at UCLA based on three pieces of data:\n",
    "\n",
    "    -GRE Scores (Test)\n",
    "    -GPA Scores (Grades)\n",
    "    -Class rank (1-4)\n",
    "\n",
    "The dataset originally came from here: http://www.ats.ucla.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calss handles all the procedure from loading data to testing accuracy\n",
    "class NN:\n",
    "    # constructor to load data from csv file\n",
    "    def __init__(self, file):\n",
    "        np.random.seed(20)  # to make sure, random is most random all the time\n",
    "        self.__data = pd.read_csv(file)\n",
    "        print('Data Loaded')\n",
    "    \n",
    "    \n",
    "    # show data\n",
    "    def getData(self):\n",
    "        return self.__data\n",
    "    \n",
    "    \n",
    "    # One-hot encode\n",
    "    # General function to apply one-hot encode on any data, just pass the dataset and col to convert to one-hot encoded vector\n",
    "    def one_hot_encoder(self, col):\n",
    "        classes = np.sort(self.__data[col].unique())\n",
    "        one_hot_vector = np.array([[0  if val != c else 1 for c in classes] for val in self.__data[col]])\n",
    "        self.__data = pd.merge(\n",
    "            self.__data,\n",
    "            pd.DataFrame(data=one_hot_vector, columns=classes),\n",
    "            left_index=True, \n",
    "            right_index=True\n",
    "        )\n",
    "        self.__data.drop([col], axis=1, inplace=True)\n",
    "        print('Done! One Hot Encoding on {} column'.format(col))\n",
    "    \n",
    "    \n",
    "    # Scaling the data\n",
    "    # We notice that the range for grades is 1.0-4.0, \n",
    "    # whereas the range for test scores is roughly 200-800, which is much larger. \n",
    "    # This means our data is skewed, and that makes it hard for a neural network to handle. \n",
    "    # Let's fit our two features into a range of 0-1, by dividing the grades by 4.0, and the test score by 800.\n",
    "    # But below will follow for any kind of data\n",
    "    def scale_data(self, *cols):\n",
    "        for col in cols:\n",
    "            self.__data[col] = self.__data[col]/np.max(self.__data[col])\n",
    "        print('Scaling Done')\n",
    "    \n",
    "    \n",
    "    # Splitting the data into training and testing \n",
    "    # further splitting the data into features(X) and lables(y)\n",
    "    # As we want random values from dataset to be selected as sample, \n",
    "    # we will use np.random.choice() to select random indices for sample\n",
    "    def train_test_split(self, label_col):\n",
    "        sample = np.random.choice(self.__data.index, size=int(len(self.__data)*0.8), replace=False)\n",
    "        train_data, test_data = self.__data.iloc[sample], self.__data.drop(sample)\n",
    "        print('No. of training data: ', len(train_data))\n",
    "        print('No. of testing data: ', len(test_data))\n",
    "        __features = train_data.drop([label_col], axis=1)\n",
    "        __labels = train_data[label_col]\n",
    "        __features_test = test_data.drop([label_col], axis=1)\n",
    "        __labels_test = test_data[label_col]\n",
    "    \n",
    "    \n",
    "    # activation function\n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    \n",
    "    # Define layers of neural network\n",
    "    # in neurons enter a list \n",
    "    # e.g. [input_features/neurons, hidden_layer_1_neurons, hidden_layer_2_neurons, output_layer_neurons]\n",
    "    def layers(self, neurons):\n",
    "        self.__neurons = neurons\n",
    "        print('Layers of Neural Network:')\n",
    "        print('Layer 1, Input Layer Neurons: {}'.format(self.__neurons[0]))\n",
    "        for i in range(1, len(self.__neurons)-1):\n",
    "            print('Layer {}, Hidden Layer {} Neurons: {}'.format(i+1, i, self.__neurons[i]))\n",
    "        print('Layer {}, Output Layer Neurons: {}'.format(len(self.__neurons), self.__neurons[-1]))\n",
    "        \n",
    "    \n",
    "    # Training Neural Network\n",
    "    def train_nn(self, epochs=1000, alpha=0.1):\n",
    "        n_records, n_features = features.shape\n",
    "        last_loss = 1\n",
    "        # initialze weights/thetas\n",
    "        self.__weights = np.random.normal(scale=1/n_features**0.5, size=n_features)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            del_w = np.zeros(weights.shape)\n",
    "            for x, y in zip(features.values, labels):\n",
    "                output = sigmoid(np.dot(x, weights))\n",
    "                # error = error_formula(y, output)\n",
    "                error = y - output\n",
    "                error_term = error_term_formula(error, output)\n",
    "                del_w += error_term*x\n",
    "            weights += alpha * del_w/n_records\n",
    "            if e % (epochs/10) == 0:\n",
    "                out = sigmoid(np.dot(features, weights))\n",
    "                loss = np.mean((out - labels)**2)\n",
    "                print('Epoch:', e)\n",
    "                if last_loss < loss:\n",
    "                    print('Train loss:', loss, \"WARNING - Loss Increasing\")\n",
    "                else:\n",
    "                    print('Train loss:', loss)\n",
    "                last_loss = loss\n",
    "                print(\"============\")\n",
    "        print('Training Finished')\n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    # forward pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "obj = NN('student_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 0.25385492547853555\n",
      "============\n",
      "Epoch: 100\n",
      "Train loss: 0.22935988613136993\n",
      "============\n",
      "Epoch: 200\n",
      "Train loss: 0.22063119597944447\n",
      "============\n",
      "Epoch: 300\n",
      "Train loss: 0.21595722627505323\n",
      "============\n",
      "Epoch: 400\n",
      "Train loss: 0.2127429996470624\n",
      "============\n",
      "Epoch: 500\n",
      "Train loss: 0.21030519363498557\n",
      "============\n",
      "Epoch: 600\n",
      "Train loss: 0.20840062878909005\n",
      "============\n",
      "Epoch: 700\n",
      "Train loss: 0.2068997653895651\n",
      "============\n",
      "Epoch: 800\n",
      "Train loss: 0.20571138936235975\n",
      "============\n",
      "Epoch: 900\n",
      "Train loss: 0.20476505908660333\n",
      "============\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "# nn hyperparameters\n",
    "epochs = 1000\n",
    "alpha = 0.1\n",
    "\n",
    "# Training function\n",
    "def train_nn(features, labels, epochs, alpha):\n",
    "    np.random.seed(20)\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = 1\n",
    "    weights = np.random.normal(scale=1/n_features**0.5, size=n_features)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        del_w = np.zeros(weights.shape)\n",
    "        for x, y in zip(features.values, labels):\n",
    "            output = sigmoid(np.dot(x, weights))\n",
    "            # error = error_formula(y, output)\n",
    "            error = y - output\n",
    "            error_term = error_term_formula(error, output)\n",
    "            del_w += error_term*x\n",
    "        weights += alpha * del_w/n_records\n",
    "        if e % (epochs/10) == 0:\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean((out - labels)**2)\n",
    "            print('Epoch:', e)\n",
    "            if last_loss < loss:\n",
    "                print('Train loss:', loss, \"WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print('Train loss:', loss)\n",
    "            last_loss = loss\n",
    "            print(\"============\")\n",
    "    print('Training Finished')\n",
    "    return weights\n",
    "\n",
    "weights = train_nn(features, labels, epochs, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11693129, -0.35895849,  0.29031635, -0.4534367 , -0.88931928,\n",
       "       -0.66072138])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "test_output = sigmoid(np.dot(features_test, weights))\n",
    "predictions = test_output > 0.5\n",
    "accuracy = np.mean(predictions == labels_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
