{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Student Admissions with Neural Networks\n",
    "In this notebook, we predict student admissions to graduate school at UCLA based on three pieces of data:\n",
    "\n",
    "    -GRE Scores (Test)\n",
    "    -GPA Scores (Grades)\n",
    "    -Class rank (1-4)\n",
    "\n",
    "The dataset originally came from here: http://www.ats.ucla.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calss handles all the procedure from loading data to testing accuracy\n",
    "class NN:\n",
    "    # constructor to load data from csv file\n",
    "    def __init__(self, file):\n",
    "        np.random.seed(20)  # to make sure, random is most random all the time\n",
    "        self.__data = pd.read_csv(file)\n",
    "        print('Data Loaded')\n",
    "    \n",
    "    \n",
    "    # show data\n",
    "    def getData(self):\n",
    "        return self.__data\n",
    "    \n",
    "    \n",
    "    # One-hot encode\n",
    "    # General function to apply one-hot encode on any data, just pass the dataset and col to convert to one-hot encoded vector\n",
    "    def one_hot_encoder(self, col):\n",
    "        classes = np.sort(self.__data[col].unique())\n",
    "        one_hot_vector = np.array([[0  if val != c else 1 for c in classes] for val in self.__data[col]])\n",
    "        self.__data = pd.merge(\n",
    "            self.__data,\n",
    "            pd.DataFrame(data=one_hot_vector, columns=classes),\n",
    "            left_index=True, \n",
    "            right_index=True\n",
    "        )\n",
    "        self.__data.drop([col], axis=1, inplace=True)\n",
    "        print('Done! One Hot Encoding on {} column'.format(col))\n",
    "    \n",
    "    \n",
    "    # Scaling the data\n",
    "    # We notice that the range for grades is 1.0-4.0, \n",
    "    # whereas the range for test scores is roughly 200-800, which is much larger. \n",
    "    # This means our data is skewed, and that makes it hard for a neural network to handle. \n",
    "    # Let's fit our two features into a range of 0-1, by dividing the grades by 4.0, and the test score by 800.\n",
    "    # But below will follow for any kind of data\n",
    "    def scale_data(self, *cols):\n",
    "        for col in cols:\n",
    "            self.__data[col] = self.__data[col]/np.max(self.__data[col])\n",
    "        print('Scaling Done')\n",
    "    \n",
    "    \n",
    "    # Splitting the data into training and testing \n",
    "    # further splitting the data into features(X) and lables(y)\n",
    "    # As we want random values from dataset to be selected as sample, \n",
    "    # we will use np.random.choice() to select random indices for sample\n",
    "    def train_test_split(self, label_col):\n",
    "        self.__output_neurons = len(label_col)\n",
    "        sample = np.random.choice(self.__data.index, size=int(len(self.__data)*0.8), replace=False)\n",
    "        train_data, test_data = self.__data.iloc[sample], self.__data.drop(sample)\n",
    "        print('No. of rows in training data: ', len(train_data))\n",
    "        print('No. of rows in testing data: ', len(test_data))\n",
    "        self.__features = train_data.drop(label_col, axis=1)\n",
    "        self.__labels = np.array(train_data[label_col])\n",
    "        self.__features_test = test_data.drop(label_col, axis=1)\n",
    "        self.__labels_test = test_data[label_col]\n",
    "    \n",
    "    \n",
    "    # activation function\n",
    "    # previous activation is inputs and weights is current thetas\n",
    "    # x = np.dot(inputs, weight)\n",
    "    # or x = np.dot(previous_activation, current_thetas)\n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    \n",
    "    # Average negative log likelihood loss function\n",
    "    def __loss(self, index):\n",
    "        return -((np.matmul(self.__labels.T, self.__activations[-1])) + ((np.matmul(1-self.__labels.T, 1-self.__activations[-1]))))\n",
    "    \n",
    "    \n",
    "    # derivative of loss function in forward pass\n",
    "    def __delForward(self, x, y):\n",
    "        return np.matmul(x.T, y*(1-y))\n",
    "    \n",
    "    \n",
    "    # Define layers of neural network\n",
    "    # in neurons enter a list \n",
    "    # e.g. [input_features/neurons, hidden_layer_1_neurons, hidden_layer_2_neurons, output_layer_neurons]\n",
    "    def layers(self, neurons):\n",
    "        self.__neurons = neurons\n",
    "        print('Layers of Neural Network:')\n",
    "        print('Layer 1, Input Layer Neurons: {}'.format(self.__neurons[0]))\n",
    "        for i in range(1, len(self.__neurons)-1):\n",
    "            print('Layer {}, Hidden Layer {} Neurons: {}'.format(i+1, i, self.__neurons[i]))\n",
    "        print('Layer {}, Output Layer Neurons: {}'.format(len(self.__neurons), self.__neurons[-1]))\n",
    "        \n",
    "    \n",
    "    # forward pass\n",
    "    def __forward(self):\n",
    "        self.__del_forward = []\n",
    "        for i in range(len(self.__neurons)-1):\n",
    "            self.__activations.append(self.__sigmoid(np.dot(self.__weights[i+1], self.__activations[i])))\n",
    "            self.__del_forward.append(self.__delForward(self.__activations[i], self.__activations[i+1]))\n",
    "    \n",
    "    \n",
    "    # reverse/backward pass\n",
    "    def __backward(self):\n",
    "        for i in range(len(self.__neurons)-2, 0, -1):\n",
    "            self.__del_backward.append(self.__weights[i+1], self.__del_backward[i+1])\n",
    "        \n",
    "    \n",
    "    # update weights\n",
    "    def __updateWeights(self):\n",
    "        for i in range(len(self.__neurons)):\n",
    "            weights[i] -= aplha*(self.__del_forward*sel.__del_backward.T) \n",
    "        \n",
    "    \n",
    "    # return weights\n",
    "    def getWeights(self):\n",
    "        return self.__weights\n",
    "    \n",
    "    \n",
    "    # Training Neural Network\n",
    "    def train_nn(self, epochs=1000, alpha=0.1, batch_size=64):\n",
    "        # n_records, n_features = features.shape\n",
    "        last_loss = None\n",
    "        self.__epochs = []\n",
    "        self.__loss = []\n",
    "        \n",
    "        # initialze weights/thetas for all the layers\n",
    "        self.__weights = [] \n",
    "        for i in range(len(self.__neurons)-1):\n",
    "            self.__weights.append(np.random.normal(scale=self.__neurons[i]**-.5, size=(self.__neurons[i], self.__neurons[i+1])))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            indices = np.random.randint(0, self.__features.shape[0], size=batch_size)\n",
    "            \n",
    "            # forward\n",
    "            self.__activations = [np.array(self.__features.iloc[indices])]\n",
    "            self.__forward()\n",
    "            \n",
    "            # backward\n",
    "            self.__E = np.mean(self.__activations[-1] - labels[indices])\n",
    "            self.__del_backward = [self.__E, self.__E*self.__weights[-1]]\n",
    "            self.__backward()\n",
    "            self.__updateWeights()\n",
    "            \n",
    "            # loss\n",
    "            self.__epochs.append(e)\n",
    "            self.__loss.append(__loss(indices))\n",
    "            \n",
    "            if e % (epochs/10) == 0:\n",
    "                print('Epoch:', e)\n",
    "                if e == 0:\n",
    "                    last_loss = self.__loss[e]\n",
    "                if last_loss < self.__loss[e]:\n",
    "                    print('Train loss:', self.__loss[e], \"WARNING - Loss Increasing\")\n",
    "                else:\n",
    "                    print('Train loss:', self.__loss[e])\n",
    "                last_loss = self.__loss[e]\n",
    "                print(\"============\")\n",
    "        print('Training Finished')\n",
    "        \n",
    "    \n",
    "    # Test - Accuracy\n",
    "    def accuracy(self):\n",
    "        test_output = sigmoid(np.dot(self.__features_test, self.__weights))\n",
    "        predictions = test_output > 0.5\n",
    "        accuracy = np.mean(predictions == self.__labels_test)\n",
    "        print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
    "        \n",
    "    \n",
    "    # Plot loss vs iterations\n",
    "    def plot(self):\n",
    "        plt.plot(self.__epochs, self.__loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Done! One Hot Encoding on rank column\n",
      "Scaling Done\n",
      "No. of rows in training data:  320\n",
      "No. of rows in testing data:  80\n",
      "Layers of Neural Network:\n",
      "Layer 1, Input Layer Neurons: 64\n",
      "Layer 2, Hidden Layer 1 Neurons: 64\n",
      "Layer 3, Hidden Layer 2 Neurons: 64\n",
      "Layer 4, Output Layer Neurons: 1\n"
     ]
    }
   ],
   "source": [
    "obj = NN('student_data.csv')\n",
    "obj.one_hot_encoder('rank')\n",
    "obj.scale_data(['gre', 'gpa'])\n",
    "obj.train_test_split(['admit'])\n",
    "obj.layers([64, 64, 64, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NN' object has no attribute '_NN__del_forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-226-3fd460562556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-224-0b7ce3097922>\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(self, epochs, alpha, batch_size)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__activations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-224-0b7ce3097922>\u001b[0m in \u001b[0;36m__forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__neurons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__activations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__activations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__del_forward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__delForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__activations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__activations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NN' object has no attribute '_NN__del_forward'"
     ]
    }
   ],
   "source": [
    "obj.train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
